{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136f03d1-efbf-49be-a1dc-af590f35426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/common/software/nersc9/matlab/R2023b/bin/matlab\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Make the 'module' command available (Lmod or Environment Modules)\n",
    "if [ -f /etc/profile.d/lmod.sh ]; then source /etc/profile.d/lmod.sh; fi\n",
    "if [ -f /etc/profile.d/modules.sh ]; then source /etc/profile.d/modules.sh; fi\n",
    "# Fallback some clusters use:\n",
    "if [ -f /usr/share/Modules/init/bash ]; then source /usr/share/Modules/init/bash; fi\n",
    "\n",
    "module load matlab          # or: module load matlab/R2024a\n",
    "which matlab                # sanity check\n",
    "\n",
    "# cd to your working dir if needed:\n",
    "# cd /global/u2/d/devard/dl\n",
    "\n",
    "matlab -nodisplay -nosplash -nodesktop -batch \\\n",
    "\"try, elevation=0; southern=false; surf='surf_mete.mat'; prof='morning_prof.mat'; process_input(elevation, southern, surf, prof); catch e, disp(getReport(e)); exit(1); end; exit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8045aca6-63ea-49fb-ae8b-8060d63c86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 788us/step\n",
      "32/32 [==============================] - 0s 893us/step\n",
      "32/32 [==============================] - 0s 826us/step\n",
      "24/24 [==============================] - 0s 724us/step\n",
      "32/32 [==============================] - 0s 806us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 781us/step\n",
      "32/32 [==============================] - 0s 737us/step\n",
      "24/24 [==============================] - 0s 723us/step\n",
      "32/32 [==============================] - 0s 861us/step\n",
      "32/32 [==============================] - 0s 778us/step\n",
      "32/32 [==============================] - 0s 748us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "24/24 [==============================] - 0s 756us/step\n",
      "32/32 [==============================] - 0s 973us/step\n",
      "32/32 [==============================] - 0s 788us/step\n",
      "32/32 [==============================] - 0s 684us/step\n",
      "32/32 [==============================] - 0s 765us/step\n",
      "24/24 [==============================] - 0s 725us/step\n",
      "32/32 [==============================] - 0s 898us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 844us/step\n",
      "32/32 [==============================] - 0s 705us/step\n",
      "24/24 [==============================] - 0s 717us/step\n",
      "32/32 [==============================] - 0s 746us/step\n",
      "32/32 [==============================] - 0s 764us/step\n",
      "32/32 [==============================] - 0s 725us/step\n",
      "32/32 [==============================] - 0s 679us/step\n",
      "24/24 [==============================] - 0s 675us/step\n",
      "32/32 [==============================] - 0s 785us/step\n",
      "32/32 [==============================] - 0s 778us/step\n",
      "32/32 [==============================] - 0s 752us/step\n",
      "32/32 [==============================] - 0s 698us/step\n",
      "24/24 [==============================] - 0s 866us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 782us/step\n",
      "32/32 [==============================] - 0s 729us/step\n",
      "32/32 [==============================] - 0s 796us/step\n",
      "24/24 [==============================] - 0s 751us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 831us/step\n",
      "32/32 [==============================] - 0s 733us/step\n",
      "24/24 [==============================] - 0s 759us/step\n",
      "32/32 [==============================] - 0s 795us/step\n",
      "32/32 [==============================] - 0s 765us/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 903us/step\n",
      "24/24 [==============================] - 0s 728us/step\n",
      "[        nan 81.75000275 81.89548069 ...         nan         nan\n",
      "         nan]\n",
      "(210, 4758)\n",
      "  1/149 [..............................] - ETA: 15s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/nersc/pe/conda-envs/24.1.0/python-3.11/nersc-python/lib/python3.11/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 0s 888us/step\n",
      "149/149 [==============================] - 0s 778us/step\n",
      "149/149 [==============================] - 0s 986us/step\n",
      "Saved: ./output/dnn_output.mat\n",
      "Saved: ./output/dnn_output.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "rescale = 0  # set to 1 to refit the scaler if current meteorology ranges differ significantly from the training data\n",
    "\n",
    "def build_cldtrain(y_train_pred, y_train_pred1, y_train_pred2, pp, cldvel):\n",
    "    \"\"\"\n",
    "    Convert model predictions into PBL & cloud profile arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train_pred : array_like, shape (LM*13, F) with F >= 12\n",
    "        Feature predictions (11 CF features in cols 0..10; thickness in col 11).\n",
    "    y_train_pred1 : array_like, shape (LM*13,) or (1, LM*13)\n",
    "        BLC flags (linearized).\n",
    "    y_train_pred2 : array_like, shape (LM*13,) or (1, LM*13)\n",
    "        Cloud-base heights (linearized).\n",
    "    pp : array_like, shape (LM*13,) or (1, LM*13)\n",
    "        Baseline PBLH (pp2), linearized.\n",
    "    cldvel : array_like, shape (V,)\n",
    "        Cloud levels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pbltrain2 : ndarray, shape (LM, 24)\n",
    "    blc_f     : ndarray, shape (LM, 24)\n",
    "    cldcbh1   : ndarray, shape (LM, 24)\n",
    "    cldcf     : ndarray, shape (LM, 24, 11)\n",
    "    cldth     : ndarray, shape (LM, 24)\n",
    "    cldtrain  : ndarray, shape (LM, 24, V)\n",
    "    \"\"\"\n",
    "    # Ensure arrays\n",
    "    y_train_pred  = np.asarray(y_train_pred,  dtype=float)\n",
    "    y_train_pred1 = np.asarray(y_train_pred1, dtype=float).ravel()\n",
    "    y_train_pred2 = np.asarray(y_train_pred2, dtype=float).ravel()\n",
    "    pp            = np.asarray(pp,            dtype=float).ravel()\n",
    "    cldvel        = np.asarray(cldvel,        dtype=float).ravel()\n",
    "\n",
    "    # Sizes\n",
    "    n1 = y_train_pred1.size\n",
    "    if n1 % 13 != 0:\n",
    "        raise ValueError(\"length(y_train_pred1) must be a multiple of 13\")\n",
    "    lm = n1 // 13\n",
    "    V  = cldvel.size\n",
    "\n",
    "    # Helper to place a linearized vector into rows 12:24 of a (24 x LM) matrix, MATLAB-style (Fortran order)\n",
    "    def place_into_24xLM(vec):\n",
    "        vec = np.asarray(vec, dtype=float).ravel()\n",
    "        if vec.size != n1:\n",
    "            raise ValueError(\"Input vector length mismatch.\")\n",
    "        M = np.full((24, lm), np.nan, dtype=float)\n",
    "        M[11:24, :] = np.reshape(vec, (13, lm), order='F')  # rows 12..24 in MATLAB\n",
    "        return M.T  # -> (LM, 24)\n",
    "\n",
    "    # 1) PBL matrix (pp)\n",
    "    pbltrain2 = place_into_24xLM(pp)\n",
    "\n",
    "    # 2) BLC flags\n",
    "    blc_f = place_into_24xLM(y_train_pred1)\n",
    "\n",
    "    # 3) Cloud-base heights\n",
    "    cldcbh1 = place_into_24xLM(y_train_pred2)\n",
    "\n",
    "    # 4) Cloud frequency for 11 features (cols 0..10 of y_train_pred)\n",
    "    cldcf = np.full((lm, 24, 11), np.nan, dtype=float)\n",
    "    for k in range(11):\n",
    "        vec = np.asarray(y_train_pred[:, k], dtype=float).ravel()\n",
    "        Bk = np.full((24, lm), np.nan, dtype=float)\n",
    "        Bk[11:24, :] = np.reshape(vec, (13, lm), order='F')\n",
    "        cldcf[:, :, k] = Bk.T  # (LM, 24)\n",
    "\n",
    "    # 5) Cloud thickness from feature 12 (col index 11)\n",
    "    vec_th = np.asarray(y_train_pred[:, 11], dtype=float).ravel()\n",
    "    T = np.full((24, lm), np.nan, dtype=float)\n",
    "    T[11:24, :] = np.reshape(vec_th, (13, lm), order='F')\n",
    "    cldth = T.T  # (LM, 24)\n",
    "\n",
    "    # 6) Assemble final cloud train (LM x 24 x V)\n",
    "    thresh = 0.5\n",
    "    cldtrain = np.full((lm, 24, V), np.nan, dtype=float)\n",
    "\n",
    "    for i in range(lm):\n",
    "        for j in range(24):\n",
    "            # If BLC flag < threshold => zeros; else construct profile\n",
    "            if blc_f[i, j] < thresh:\n",
    "                cldtrain[i, j, :] = 0.0\n",
    "                continue\n",
    "\n",
    "            cb = cldcbh1[i, j]\n",
    "            th = cldth[i, j]\n",
    "\n",
    "            # Guard against NaNs (MATLAB would error downstream; choose zeros here)\n",
    "            if not np.isfinite(cb) or not np.isfinite(th):\n",
    "                cldtrain[i, j, :] = 0.0\n",
    "                continue\n",
    "\n",
    "            if th >= 50:\n",
    "                ct = cb + th - 1.0\n",
    "                ix = int(np.argmin(np.abs(cldvel - cb)))  # nearest to base\n",
    "                iy = int(np.argmin(np.abs(cldvel - ct)))  # nearest to top\n",
    "                if iy < ix:\n",
    "                    ix, iy = iy, ix\n",
    "                ix = max(0, min(ix, V - 1))\n",
    "                iy = max(0, min(iy, V - 1))\n",
    "\n",
    "                c = np.squeeze(cldcf[i, j, :]).astype(float)  # length 11\n",
    "                a = np.zeros(V, dtype=float)\n",
    "\n",
    "                if iy >= ix:\n",
    "                    # 11 points spanning [ix, iy], like ix : (iy-ix)/10 : iy in MATLAB\n",
    "                    xi = np.linspace(ix, iy, 11)\n",
    "                    grid = np.arange(ix, iy + 1)\n",
    "                    # Linear interpolation of the 11 CF values across the index span\n",
    "                    a[ix:iy + 1] = np.interp(grid, xi, c)\n",
    "                cldtrain[i, j, :] = a\n",
    "            else:\n",
    "                # thin cloud: fill two levels with mean CF\n",
    "                m = int(np.argmin(np.abs(cldvel - cb)))\n",
    "                a = np.squeeze(cldcf[i, j, :]).astype(float)  # length 11\n",
    "                mean_val = np.mean(a)  # match MATLAB mean (NaNs propagate)\n",
    "                z = np.zeros(V, dtype=float)\n",
    "                end_idx = min(m + 2, V)  # set indices m and m+1 if available\n",
    "                z[m:end_idx] = mean_val\n",
    "                cldtrain[i, j, :] = z\n",
    "\n",
    "    return pbltrain2, blc_f, cldcbh1, cldcf, cldth, cldtrain\n",
    "\n",
    "MODELDIR = './models'\n",
    "\n",
    "with open(os.path.join(MODELDIR, 'pblh_candidate.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    y_scaler = data['y_scaler']\n",
    "    x_scaler = data['x_scaler']\n",
    "    importances = data['importances']\n",
    "    \n",
    "features_to_keep = [i for i, importance in enumerate(importances) if importance < 0]\n",
    "x_scaler_pblh=x_scaler\n",
    "\n",
    "print(len(features_to_keep))\n",
    "\n",
    "# Use only the models that were actually selected/used\n",
    "selected_model_filenames = [\n",
    "    'possible_candidate1.h5',\n",
    "    'model_ff_1.h5',\n",
    "    'model_ff_2.h5',\n",
    "    'model_fff_0.h5',\n",
    "    'model_fff_1.h5',\n",
    "    'model_fff_3.h5',\n",
    "    'model_fff_5.h5',\n",
    "    'model_fff_8.h5',\n",
    "    'model_fff_15.h5',\n",
    "    'model_fff_17.h5',\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "data = sio.loadmat('input_for_dnn.mat', variable_names=['xx','xx1'])\n",
    "xx = data['xx']\n",
    "x_train1 = xx.T\n",
    "        \n",
    "# Scale the data\n",
    "if rescale == 1:\n",
    "    x_scaler_pblh = StandardScaler().fit(x_train1)  # <-- only when rescale==1\n",
    "x_trainn = x_scaler_pblh.transform(x_train1)\n",
    "    \n",
    "# Define the batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Initialize an array to store predictions from selected models\n",
    "y_pred_originalall = np.zeros((len(selected_model_filenames), x_trainn.shape[0], 1))\n",
    "\n",
    "# Iterate over each selected model\n",
    "for idx, model_filename in enumerate(selected_model_filenames):\n",
    "    # Load the model\n",
    "    model = load_model(os.path.join(MODELDIR, model_filename))\n",
    "\n",
    "    y_pred_list = []\n",
    "    for i in range(0, x_trainn.shape[0], batch_size):\n",
    "        batch = x_trainn[i:i+batch_size, features_to_keep]  # Adjust features_to_keep if needed\n",
    "        batch_pred = model.predict(batch)\n",
    "        y_pred_list.append(batch_pred)\n",
    "\n",
    "    y_pred_combined = np.vstack(y_pred_list)\n",
    "    y_pred_original = y_scaler.inverse_transform(y_pred_combined)  # Assuming y_scaler is already fitted\n",
    "    y_pred_originalall[idx, :] = y_pred_original\n",
    "    \n",
    "    y_pred_originalall1 = y_pred_originalall\n",
    "    \n",
    "    \n",
    "    xx1 = data['xx1']\n",
    "pp1 = np.squeeze(y_pred_originalall1)  # Remove single-dimensional entries\n",
    "pp2 = np.mean(pp1, axis=0)  # Calculate the mean along the default axis\n",
    "pp3 = np.insert(pp2[:-1], 0, np.nan)\n",
    "oo = np.column_stack((pp3, pp2))\n",
    "oo1=oo.T\n",
    "oo2 = np.vstack((xx1, oo1))  # Vertically stack xx and xx1, then transpose\n",
    "oo3 = oo1 - xx1[12:14,:]  # Difference between column pairs\n",
    "xx2 = np.vstack((oo2, oo3))  # Vertically stack xx and xx1, then transpose\n",
    "#xx = xx2[:, 1:]\n",
    "xx=xx2\n",
    "# Calculate the mean of all variables (columns)\n",
    "means = np.mean(xx, axis=0)\n",
    "print(means)\n",
    "print(xx.shape)\n",
    "\n",
    "model1 = load_model(os.path.join(MODELDIR, 'type_model_f1.h5'))\n",
    "modelcf = load_model(os.path.join(MODELDIR, 'cfra_all.h5'))\n",
    "modelcbh = load_model(os.path.join(MODELDIR, 'cbh_ff.h5'))\n",
    "\n",
    "with open(os.path.join(MODELDIR, 'cfra_all.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    important_features_indices = data['important_features_indices']\n",
    "    features_to_keep = data['features_to_keep']\n",
    "    x_scaler_cf = data['x_scaler_cf']\n",
    "    y_scaler_cf = data['y_scaler_cf']\n",
    "    x_scaler_type = data['x_scaler_type']\n",
    "\n",
    "with open(os.path.join(MODELDIR, 'cbhf_all.pkl'), 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    features_cbh = data['features_cbh']\n",
    "    x_scaler_cbh = data['x_scaler_cbh']\n",
    "    y_scaler_cbh = data['y_scaler_cbh']\n",
    "    \n",
    "x_train1 = xx.T\n",
    "\n",
    "if rescale == 1:\n",
    "    x_scaler_type = StandardScaler().fit(x_train1)\n",
    "x_trainn = x_scaler_type.transform(x_train1)\n",
    "y_train_pred1 = model1.predict(x_trainn[:, important_features_indices])\n",
    "y_train_pred1=y_train_pred1.T\n",
    "combined_matrix = np.vstack((xx, y_train_pred1))\n",
    "x_train1 = combined_matrix.T\n",
    "\n",
    "if rescale == 1:\n",
    "    x_scaler_cbh = StandardScaler().fit(x_train1)\n",
    "x_trainn = x_scaler_cbh.transform(x_train1)\n",
    "y_train_pred = modelcbh.predict(x_trainn[:,features_cbh])\n",
    "y_train_pred2 = y_scaler_cbh.inverse_transform(y_train_pred)\n",
    "\n",
    "if rescale == 1:\n",
    "    x_scaler_cf = StandardScaler().fit(x_train1)\n",
    "x_trainn = x_scaler_cf.transform(x_train1)\n",
    "y_train_pred = modelcf.predict(x_trainn[:,features_to_keep])\n",
    "y_train_pred = y_scaler_cf.inverse_transform(y_train_pred)\n",
    "\n",
    "# ---- prep shapes without any .mat I/O ----\n",
    "y_train_pred  = np.asarray(y_train_pred)\n",
    "y_train_pred1 = np.asarray(y_train_pred1).ravel()\n",
    "y_train_pred2 = np.asarray(y_train_pred2).ravel()\n",
    "pp            = np.asarray(pp2).ravel()\n",
    "\n",
    "# Normalize shapes/orientation just in case\n",
    "n1 = np.size(y_train_pred1)\n",
    "y_train_pred1 = np.asarray(y_train_pred1).ravel()\n",
    "y_train_pred2 = np.asarray(y_train_pred2).ravel()\n",
    "pp            = np.asarray(pp).ravel()\n",
    "\n",
    "# If y_train_pred is transposed (F x N), fix it to (N x F)\n",
    "if y_train_pred.shape[0] != n1 and y_train_pred.shape[1] == n1:\n",
    "    y_train_pred = y_train_pred.T\n",
    "\n",
    "assert y_train_pred.shape[0] == n1, f\"y_train_pred first dim {y_train_pred.shape[0]} != {n1}\"\n",
    "assert y_train_pred.shape[1] >= 12, \"y_train_pred must have at least 12 columns (11 CF + thickness).\"\n",
    "\n",
    "# --- 2) Define cldvel like MATLAB: 30:30:150*30 ---\n",
    "cldvel = np.arange(30, 150*30 + 1, 30, dtype=float)  # 30, 60, ..., 4500\n",
    "\n",
    "# --- 3) Run build_cldtrain ---\n",
    "pbltrain2, blc_f, cldcbh1, cldcf, cldth, cldtrain = build_cldtrain(\n",
    "    y_train_pred, y_train_pred1, y_train_pred2, pp, cldvel\n",
    ")\n",
    "\n",
    "\n",
    "# --- Trim helpers (keep last 14 hours: 11..24) ---\n",
    "def trim14(x):\n",
    "    if x.ndim == 2:   # (LM, 24)\n",
    "        return x[:, -14:]\n",
    "    elif x.ndim == 3: # (LM, 24, K)\n",
    "        return x[:, -14:, :]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ndim={x.ndim} for trim14\")\n",
    "\n",
    "# Apply trim to all hour-sized arrays\n",
    "pblh_14   = trim14(pbltrain2)     # (LM,14)\n",
    "blc_14    = trim14(blc_f)         # (LM,14)\n",
    "cbh_14    = trim14(cldcbh1)       # (LM,14)\n",
    "cldcf_14  = trim14(cldcf)         # (LM,14,11)\n",
    "cldth_14  = trim14(cldth)         # (LM,14)\n",
    "cldprof_14= trim14(cldtrain)      # (LM,14,150)\n",
    "\n",
    "SURF_PATH = 'input_for_dnn.mat'\n",
    "surf = sio.loadmat(SURF_PATH, squeeze_me=True)\n",
    "date_vec = np.ravel(surf['date']).astype(np.int64)\n",
    "\n",
    "# --- Save outputs to ./output ---\n",
    "outdir = './output'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# MAT\n",
    "mat_path = os.path.join(outdir, 'dnn_output.mat')\n",
    "payload = {\n",
    "    'pblh'         : pblh_14,\n",
    "    'blc_occurrence': blc_14,\n",
    "    'cbh'          : cbh_14,\n",
    "    'cldcf'        : cldcf_14,\n",
    "    'cldthick'     : cldth_14,\n",
    "    'cld_prof'     : cldprof_14,\n",
    "    'height'       : cldvel,      # unchanged vertical grid\n",
    "    'date' : date_vec,\n",
    "}\n",
    "sio.savemat(mat_path, payload)\n",
    "print(f\"Saved: {mat_path}\")\n",
    "\n",
    "# NetCDF\n",
    "nc_path = os.path.join(outdir, 'dnn_output.nc')\n",
    "LM, H14 = pblh_14.shape\n",
    "V = cldprof_14.shape[2]\n",
    "CFBINS = cldcf_14.shape[2]\n",
    "\n",
    "with Dataset(nc_path, 'w', format='NETCDF4') as nc:\n",
    "    # Dimensions\n",
    "    nc.createDimension('time', LM)\n",
    "    nc.createDimension('hour', H14)       # 14 hours\n",
    "    nc.createDimension('height', V)       # 150\n",
    "    nc.createDimension('cfbin', CFBINS)   # 11\n",
    "\n",
    "    # Coordinates\n",
    "    v_time   = nc.createVariable('time', 'i4', ('time',))\n",
    "    v_height = nc.createVariable('height', 'f4', ('height',))\n",
    "    v_time[:] = np.arange(LM, dtype=np.int32)\n",
    "    v_height[:] = cldvel.astype(np.float32)\n",
    "\n",
    "    if date_vec is not None:\n",
    "        v_date = nc.createVariable('date_yyyymmdd', 'i8', ('time',))\n",
    "        v_date[:] = date_vec.astype(np.int64)\n",
    "\n",
    "    def makevar(name, dtype, dims):\n",
    "        return nc.createVariable(name, dtype, dims, zlib=True, complevel=4, shuffle=True)\n",
    "\n",
    "    makevar('pblh',          'f4', ('time','hour'))[:]         = pblh_14.astype(np.float32)\n",
    "    makevar('blc_occurrence','f4', ('time','hour'))[:]         = blc_14.astype(np.float32)\n",
    "    makevar('cbh',           'f4', ('time','hour'))[:]         = cbh_14.astype(np.float32)\n",
    "    makevar('cldcf',         'f4', ('time','hour','cfbin'))[:] = cldcf_14.astype(np.float32)\n",
    "    makevar('cldthick',      'f4', ('time','hour'))[:]         = cldth_14.astype(np.float32)\n",
    "    makevar('cld_prof',      'f4', ('time','hour','height'))[:]= cldprof_14.astype(np.float32)\n",
    "\n",
    "    # attrs\n",
    "    nc.variables['height'].units = 'm'\n",
    "    nc.variables['pblh'].units   = 'm'\n",
    "    nc.variables['cbh'].units    = 'm'\n",
    "\n",
    "print(f\"Saved: {nc_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fdacc-1438-4a0e-a599-14be06445276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
